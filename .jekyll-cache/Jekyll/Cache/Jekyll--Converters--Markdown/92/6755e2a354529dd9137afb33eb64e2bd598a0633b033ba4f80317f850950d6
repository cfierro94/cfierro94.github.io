I"7
<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<blockquote>
  <p>Paper summary: Attention is all you need , Dec. 2017. (<a href="https://arxiv.org/abs/1706.03762">link</a>)</p>
</blockquote>

<h2 id="why-is-it-important">Why is it important?</h2>

<p>This is the paper that first introduced the transformer architecture, which allowed language models to be way bigger than before thanks to its capability of being easily parallelizable. Consequently, models such as BERT and GPT achieved far better results in diverse NLP tasks.</p>

<h2 id="what-does-it-propose">What does it propose?</h2>

<p>This work proposed a network architecture to perform neural machine translation (NMT). This new model is entirely based on the attention mechanism, contrary to the standard at that point of using recurrent networks with attention. The architecture was tested in two NMT tasks and it outperformed the best existent models, in addition to using less resources. Furthermore, the model was also successfully tested in a different task (english constituency parsing).</p>

<p>The inherently sequential nature of RNNs precludes parallelization within training examples, moreover the best RNNs architectures don’t rely solely on one or a couple of hidden states, but use attention to attend to the most relevant hidden states. That’s why the architecture presented in this paper is so relevant and impactful, it’s able to achieve better results getting rid of the sequentiality of RNNs.</p>

<h2 id="how-does-it-work">How does it work?</h2>

<p>The Transformer is an encoder-decoder architecture, the encoder corresponds to the left side of the image below (Figure 1) and the decoder to the right one. In this paper the authors introduced the multi-head self-attention layer and the positional encodings used in the architecture (details in the next 2 sections).</p>

<p>Essentially, token embeddings are added with their positional encoding and used as inputs in the encoder and decoder. The encoder is composed of a stack of N=6 layers, we can see one of such layers in Figure 1. The decoder is also composed of a stack of N=6 identical layers, which has the two sub-layers of the encoder but it inserts a third sub-layer first that performs a multi-head attention over the output. Each feed-forward and multi-head self-attention layer is followed by a residual connection and a layer normalization, thus the output of each sub-layer is <script type="math/tex">\text{LayerNorm}(x+\text{SubLayer}(x))</script>.</p>

<p>Some extra details:</p>

<ul>
  <li>Byte pair encoding is used to define the tokens of the text.</li>
  <li>The feed forward network consists of two linear transformations with a ReLu in between.</li>
  <li>Regularizations:
    <ul>
      <li>Dropout on the output of each sub-layer (before it’s added and normalized).</li>
      <li>Label smoothing.</li>
    </ul>
  </li>
  <li>Beam search is used to generate the text.</li>
</ul>

<figure>
<img src="../assets/img/nlp-summary-01/architecture.png" alt="Figure 1. The Transformer architecture." style="width:60%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 1. The Transformer architecture.</figcaption>
</figure>

<h3 id="positional-encoding">Positional Encoding</h3>

<p><strong>Motivation</strong>: Since there’s no recurrence, a positional encoding vector is added to the token embedding to inject information about its position in the text.</p>

<p>The <script type="math/tex">PE(w_t)</script>, the positional encoding for the word <script type="math/tex">w</script> at position <script type="math/tex">t</script>, is a vector of dimension <script type="math/tex">d_{model}</script> equal to the embedding dimension. We compute each dimension <script type="math/tex">i</script> of this vector as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
PE_i(w_t) = \left\{\begin{array}{ll}      sin(k_j*t) & \text{if} \quad i=2j \\      cos(k_j* t) & \text{if} \quad i=2j+1 \\\end{array} \right. \\[20pt]\text{where,} \quad k_j = \frac{1}{10000^{2i/d_{model}}} %]]></script>

<p>Which give as,</p>

<script type="math/tex; mode=display">PE(w_t) = \begin{bmatrix}sin(k_0t)\\cos(k_0t)\\... \\sin(k_{d/2}t)\\cos(k_{d/2}t)\end{bmatrix}</script>

<p>We can think of this as a bit representation of numbers, with each dimension of the vector as a bit, since each bit changes periodically and we can tell that one number is bigger than another because of the bits activated and their order. More of this intuition in <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">this blog post</a>.</p>

<p>The authors showed that learned positional embeddings were as good as these in their test sets. But they think sin/cos is better because it could extrapolate better.</p>

<h3 id="attention-layer">Attention Layer</h3>

<p>Instead of just having one attention layer the authors found beneficial to linearly project the attention mechanism, thus performing multiple attentions in parallel (Image 2). Each of the attention mechanisms is a scaled dot-product attention explained below.</p>

<figure>
<img src="../assets/img/nlp-summary-01/attentions.png" alt="Figure 2. Attention mechanisms of the Transformer." style="width:70%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 2. Attention mechanisms of the Transformer.</figcaption>
</figure>

<h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4>

<p>Attention is a function that takes a query and a set of key-value pairs as inputs, and computes a weighted sum of the values, where the weights are obtained from a compatibility function between the query and the corresponding key.</p>

<p>The specific attention used here, is called <em>scaled dot-product</em> because the compatibility function used is:</p>

<script type="math/tex; mode=display">\text{weight}(q,k_i) =\text{softmax}(\frac{q\cdot k_i}{\sqrt{d_k}}) \quad \text{where} \, q,k_i \in \!R^{d_k}</script>

<p>The authors decided to use a dot-product attention over an additive attention because it is faster to compute and more space-efficient. But it has been shown to perform worse for larger dimensions of the input (<script type="math/tex">d_k</script>), thus they added the scaling factor <script type="math/tex">\sqrt{d_k}</script> to counteract the effect of the dot product getting too large (which they suspect is the problem).</p>

<h4 id="masked-multi-head-self-attention">Masked Multi-Head Self-Attention</h4>

<p>In training we don’t want to show the complete output sentence to our model, but instead we want to present the words one by one, to not let extra information flow in the decoder. That’s why in Image 2 we see a “Mask opt.” which refers to setting those vectors to -inf, making them 0 after the softmax. Image 3 can help to understand how this affects the architecture overall.</p>

<figure>
<img src="../assets/img/nlp-summary-01/mask-transformer.png" alt="Figure 3. The Transformer architecture masking the output." style="width:70%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 3. The Transformer architecture masking the output.</figcaption>
</figure>

<h2 id="whats-next">What’s next!</h2>

<p>BERT and GPT are just some of the applications that the Transformer can have, but it has also been applied to <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html">images</a>, <a href="https://arxiv.org/abs/1710.10903]">graph networks</a>, <a href="https://arxiv.org/abs/1805.08318">GANs</a>, among others, achieving state-of-the-art results. It has also been useful to interpret a part of the models that use it (<a href="https://arxiv.org/abs/1910.05276">https://arxiv.org/abs/1910.05276</a>).</p>
:ET