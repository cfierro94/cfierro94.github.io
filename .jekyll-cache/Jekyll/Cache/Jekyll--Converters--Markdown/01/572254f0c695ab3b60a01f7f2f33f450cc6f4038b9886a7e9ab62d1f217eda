I"{A<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<blockquote>
  <p>(<a href="https://arxiv.org/abs/2004.13637">Roller et. al, 2020</a>). This paper studies in depth the performance of a chatbot based in the Transformer. It shows that it’s able to respond in a really human way, and it’s able to maintain a chit chat conversation. However, they also show that the model lacks in-depth knowledge, it forgets facts stated before and it tends to repeat what the other locutor is saying.</p>
</blockquote>

<h3 id="what-does-it-propose">What does it propose?</h3>

<p>It constructs different chatbots based on the Transformer, and it analyses different axes of developing a chatbot. It finds that:</p>

<ul>
  <li>Fine tuning in datasets that focus on personality, empathy, knowledge, etc. Makes the chatbot more human (even when using smaller models).</li>
  <li>It tries different decoding strategies, showing that beam search can be as good or better than  sampling.</li>
  <li>It presents the flaws of the developed models.</li>
</ul>

<h2 id="models">Models</h2>

<h3 id="retriever">Retriever</h3>

<p>(<a href="https://arxiv.org/pdf/1905.01969">Humeau et al., 2019</a>)</p>

<figure>
<img src="../assets/img/nlp-summary-03/poly_encoder.png" alt="Figure 1. Poly encoder architecture." style="width:45%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 1. Poly encoder architecture.</figcaption>
</figure>

<p><u>The idea</u>: given a dialogue history (context), it retrieves the next dialogue utterance by scoring a large set of candidate responses (typically all possible training responses).</p>

<p><u>How</u>: It constructs an embedding of the context (\(y_{ctxt}\)) and one for each response candidate (\(y_{cand_i}\)), to then calculate the score of each with the dot product: \(y_{cand_i}\cdot y_{ctxt}\). These embeddings representations are constructed as follows:</p>

<ol>
  <li>Right side of Figure 1: It obtains the candidates embeddings  using a transformer encoder (BERT) and an aggregator function, that can be simply taking the classifier embedding \(C\) of the output, or the average of the tokens.</li>
  <li>Left side of Figure 1: It encodes the context using another transformer and then performing \(m\) attentions (with \(m\) a hyper parameter). Each attention (see the definition <a href="https://cfierro94.github.io/nlp-summaries/attention-is-all-you-need#scaled-dot-product-attention">here</a>) uses as keys and values the transformer output and as query a learned code \(c_i\) unique for each attention. It then computes another attention on top of those embeddings, where the query is the \(y_{cand_i}\) and the keys and values are the output from the other attention \(y^i_{ctxt}\). In equations:</li>
</ol>

\[T(x) = (h_1, ..., h_N) \qquad \text{(Transformer output)}\\
y^i_{ctxt} = \sum_jw_j^{c_i}h_j \\
\text{where} \; (w_1^{c_i}, ..., w_N^{c_i}) = \text{softmax}(c_i\cdot h_1, ..., c_i\cdot h_N) \\
y_{ctxt} = \sum_iw_i y^i_{ctxt} \\
\text{where} \; (w_1, ..., w_m) = \text{softmax}(y_{cand_i}\cdot y_{ctxt}^1, ..., y_{cand_i}\cdot y_{ctxt}^m)\]

<h3 id="generator">Generator</h3>

<p>(<a href="https://arxiv.org/abs/1706.03762">Aswani et. al, 2017</a>)</p>

<p>Standard Seq2seq model, like the transformer of “Attention is all you need” (<a href="https://cfierro94.github.io/nlp-summaries/attention-is-all-you-need">summary here</a>) but way bigger (90M, 2.7B, 9.4B). In comparison, Meena Google’s chatbot (<a href="https://arxiv.org/abs/2001.09977">Adiwardana et. al, 2020</a>) has 2.7B parameters.</p>

<h3 id="retrieve-and-refine">Retrieve and refine</h3>

<p>(<a href="https://arxiv.org/abs/1808.04776">Weston et al., 2018</a>)</p>

<p>Trying to solve the problems of generator models (hallucinate knowledge, unable to read and access external knowledge, dull and repetitive responses). Here they mix the two models above appending to the input of a generator model the output of a retriever model (Figure 2), using a special separator token. They experiment with two types:</p>

<ol>
  <li><strong>Dialogue retrieval</strong>: it uses the dialog history and it produces a response (Same retriever architecture)</li>
  <li><strong>Knowledge retriever</strong>: it retrieves from a large knowledge base, where the candidates are obtained from a TF-IDF-based inverted index lookup over a Wikipedia dump. For this case a transformer is additionally trained to decide when to add the knowledge retrieval and when not to (as some contexts do not require knowledge).</li>
</ol>

<figure>
<img src="../assets/img/nlp-summary-03/retnrefine.png" alt="Figure 2. Retrieve and Refine architecture." style="width:55%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 2. Retrieve and Refine architecture.</figcaption>
</figure>

<h2 id="training-objectives">Training objectives</h2>

<ul>
  <li><u>Retriever</u>: cross entropy over the \(y_{cand_i}\) where \(y_{cand_1}\) is the score of the correct reponse and the rest are negatives.</li>
  <li><u>Generator</u>: standard maximum likelihood estimation (MLE)</li>
  <li><u>Dialogue retrieval</u>: with MLE is not clear the relation between the retrieval response and the gold label (the correct answer), it has been proven that just using MLE makes the model ignore the retrieved utterance. Thus, here they replace the gold label with the retrieved utterance \(\alpha \%\) of the times.</li>
  <li><u>Knowledge retrieval</u>: here we can simply use MLE because the fine-tuning datasets used have a clear correspondence between the correct knowledge retrieval and response.</li>
</ul>

<h3 id="unlikelihood-training">Unlikelihood training</h3>

<p>(<a href="https://arxiv.org/pdf/1908.04319">Welleck et al., 2020</a>)</p>

<p>They also tried this objective because it was created to mitigate problems of MLE when training language models, such as repetition: using the same tokens more frequently than a human, and token distribution mismatch: using low frequency tokens (more specific tokens) too rarely compared to humans.</p>

<p><u>Main idea:</u> to decrease the model’s probability of certain tokens, called negative candidates \(C_t\). To do this, we’ll add an expression to the MLE loss that we’ll take these candidates into account, this is what we call unlikelihood loss:</p>

\[-\sum_{c\in C_t}\log(1-p_\theta(c|x_{&lt; t}))\]

<p>Where \(p_\theta\) is our language model predictions, and \(x_{&lt; t}\) is a sequence of \(t\) tokens. As typically with losses, we have a negative logarithm that we will minimize, which is equivalent to maximize the logarithm, therefore we’ll be maximizing whatever is inside it. Since in this case we don’t want the negative candidates (\(c\)) to be highly probable, we’ll maximize the likelihood of not having them, so we’ll maximize \(1-p_\theta(...)\).</p>

<p>Thus, the actual training objective will be a mixture (gated by \(\alpha\) hyper parameter) of the unlikelihood of bad candidates and the likelihood of the next token:</p>

\[-\alpha \underbrace{-\sum_{c\in C_t}\log(1-p_\theta(c|x_{&lt;t}))}_{\text{unlikelihood}} \underbrace{- \log(p_\theta(x_t|x_{&lt;t}))}_{\text{likelihood}}\]

<p>In the paper they defined the set of bad candidates as the tokens that were generated more frequently by the model than by humans. To measure these frequencies they kept a running count of the distribution of the tokens generated by the model and they compared it to the distribution of the gold responses.</p>

<h2 id="decoding">Decoding</h2>

<p>They tried different decoding strategies:</p>

<ol>
  <li><strong>Beam search</strong> (<a href="https://cfierro94.github.io/nlp-deep-dive/attention-is-all-you-need#beam-search">summary here</a>)</li>
  <li><strong>Top-k sampling</strong>: at each time step the word \(i\) is selected by sampling from the k (=10) most likely candidates from the model distribution.</li>
  <li><strong>Sample-and-rank sampling</strong>: \(N\) independent sentences are sampled (following the model probabilities) and then the one with the highest probability is selected.</li>
</ol>

<p>They also tried additional constraints for the decoding process:</p>

<ol>
  <li><strong>Minimum length</strong>: Force the model to produce an answer of a defined length.</li>
  <li><strong>Predictive length</strong>: Predict (with a retriever model) the minimum length of the answer (e.g., &lt;10, &lt;20, &lt;30, &gt;30 tokens). And then we do the same as in 1.</li>
  <li><strong>Beam blocking</strong>: Force the model to not produce in the next utterance a trigram (group of 3 words) that’s in the input or in the utterance itself. That can be achieved by setting to 0 the probability of the words that would create a trigram that already exists.</li>
</ol>

<h2 id="training-data">Training data</h2>

<p>Train:</p>

<ul>
  <li>Pushshift.io Reddit: reddit discussions covering a <strong>vast range of topics</strong>.</li>
</ul>

<p>Two-way conversational data to fine tune the models:</p>

<ul>
  <li>ConvAI2 dataset (<a href="https://arxiv.org/abs/1801.07243">Zhang et al., 2018</a>) focuses on <strong>personality</strong> and engaging the other speaker. It gives a persona description to the speaker (which is concatenated to the history to use it as input in the model).</li>
  <li>Empathetic Dialogues (<a href="https://arxiv.org/abs/1811.00207">Rashkin et al., 2018</a>) focuses on <strong>empathy</strong>.</li>
  <li>Wizard of Wikipedia (<a href="https://arxiv.org/abs/1811.01241">Dinan et al., 2018</a>) focuses on <strong>knowledge</strong>.</li>
  <li>Blended Skill Talk (<a href="https://arxiv.org/pdf/2004.08449">Smith et al., 2020</a>) provides a dataset that focuses on blending all the previous skills. This is constructed with one human speaking freely (using its persona) and the other one guided, that is he/she has to choose an utterance response from 3 different possibilities constructed by a model trained in each of the three previous datasets.</li>
</ul>

<h2 id="evaluation-methods">Evaluation methods</h2>

<h4 id="acute-eval">ACUTE-eval</h4>

<p>This is a manual evaluation where two different dialogues (between a model an a human) are presented to a person that needs to answer:</p>

<ul>
  <li>“Who would you prefer to talk to for a long conversation?” (Engagingness)</li>
  <li>“Which speaker sounds more human?” (Humanness)</li>
</ul>

<h4 id="self-chat-acute-eval">Self-Chat ACUTE-Eval</h4>

<p>Same as ACUTE-eval but we the dialogues are generated by the model talking to itself instead of a human.</p>

<h2 id="results">Results</h2>

<p>The results are comparisons in the votes given for each question presented above. In the paper they say that some results are “not significant”, which basically means that given the amount of answers collected and the votes in each side, is not certain if one is better than the other, as in the difference could be noise of the measure.</p>

<h3 id="results-of-self-chat-acute-eval">Results of Self-Chat ACUTE-Eval</h3>

<ul>
  <li>When comparing the <strong>3 models</strong> using standard beam search (beam size 10, no minimum beam decoding constraint, but with context and response 3-gram blocking), the results are Retriever &gt; RetNRef &gt; Generator.</li>
  <li>When comparing <u>decoding choices</u>:
    <ul>
      <li>In terms of <strong>minimum length</strong>: The best results were encountered when setting a minimum length of 20 or 40, or when predicting the minimum length using the buckets 10,20,30,40.</li>
      <li>In terms of <strong>beam blocking</strong>: Blocking both context and response 3-grams during generation gives highest scores, however, the differences were not significant.</li>
      <li>Comparing different <strong>beam sizes and sampling methods</strong>, it appears that a beam value of 10 is superior to 1 or 30, and a 10 size beam is on par with sampling methods.</li>
    </ul>
  </li>
  <li>Larger models perform better</li>
  <li>Fine tuning in the 4 extra datasets give huge improvements</li>
  <li>Using the persona context (description about a specific persona) after having fine tuning gives a little improvement compared to not using them.</li>
  <li>Unlikelihood training has a small gain (although it’s not statistically significant). Notice that the conversations in these experiments are short so maybe the advantages of this training objective are not totally exploited.</li>
</ul>

<h3 id="results-of-acute-eval">Results of ACUTE-eval</h3>

<p>Results of conversations of 14 turns between humans-chatbot.</p>

<ul>
  <li>Comparing the 3 models with the improved decoding strategy (beam size 10, minimum length 20, blocking context and response) the results were: RetNRef &gt; Generator &gt; Retriever.</li>
  <li>Comparing to Meena (<a href="https://arxiv.org/abs/2001.09977">Adiwardana et. al, 2020</a>):
    <ul>
      <li>In the engagingness question the generative model of the same size is better 75% of the times.</li>
      <li>In the humanness question the generative model of the same size is better 65% of the times, and the generative of the same size trained with unlikelihood is better 70% of the times.</li>
    </ul>
  </li>
</ul>

<p>When comparing one human-chatbot dialogue to a human-human dialogue: the results that are statistically significant show that the models in this paper are 37% of the times better than human-human dialogues in the engagingness question. Additionally, the generative model is 49% of the times better in the same question, but this is not statistically significant. Even though this result sounds promising, the model is not this close to a human dialogue, below we can see the flaws that the authors presented and that are not really measured by this evaluation.</p>

<h3 id="failure-cases">Failure cases</h3>

<ul>
  <li><strong>Words repetition.</strong> The minimum length helps to create more detailed messages, but the core problem still remains. Some 3-grams were over-expressed compared to human-human conversations, such as: “do you like”, “lot of fun”, “have any hobbies”.
    <ul>
      <li>Evaluation problem: the current evaluation does not seem to expose this as boring because the conversations are short and are evaluated separately.</li>
    </ul>
  </li>
  <li><strong>Ideas repetition.</strong> Beam blocking helps with this issue, but it can be seen that the model has a tendency to repeat what the other part says, if the human says he/she has a dog then the bot repeats that it has one too, the chatbot likes the same bands as you, etc.</li>
  <li><strong>Forgetfulness.</strong> The model does not link correctly to past statements, for example you tell the model you have a dog, but then later in the conversation it asks what pets do you have.</li>
  <li><strong>Contradiction.</strong> It makes contradictions linked to overall knowledge, for example it says it lives in the MidWest and then it specifies it lives in Georgia (which is not in the midwest).</li>
  <li><strong>Knowledge</strong>. They observed that the models often switch topics, avoiding the challenge of going “deeper”. The reading of knowledge only hurt the model in the evaluation setup, possibly due to:
    <ul>
      <li>The model attempts to use knowledge when there is no need, or using it incorrectly.</li>
      <li>Evaluation problem: deeper knowledge is not really required in this setup, since the dialogues are short and tend to cover only shallow topics whereby the speakers get to know each other.</li>
    </ul>
  </li>
  <li><strong>Context length.</strong> The models in this paper have a hard limit of 128 tokens, there’s been some research in this problem but it would need another setup to be evaluated (with dialogues longer than 14 turns).</li>
  <li><strong>Deeper understanding.</strong> These models cannot be taught a concept through further conversation, so as-is they will always be stunted. See fun examples in Figure 3.</li>
</ul>

<figure>
<img src="../assets/img/nlp-summary-03/dialogues.png" alt="Figure 3. Pun dialogues failures." style="width:100%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 3. Pun dialogues failures.</figcaption>
</figure>

<h2 id="whats-next">What’s next!</h2>

<p>This paper shows a really robust and advanced chatbot, however it also presented a lot of remaining challenges to really be near a human-like bot. If you’re interested in one of the challenges presented in this summary go and read the paper! 😀 Because it cites work that is trying to overcome these issues.</p>
:ET