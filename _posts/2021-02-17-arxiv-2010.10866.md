---
layout: post
title:  "PARENTing via Model-Agnostic Reinforcement Learning to Correct Pathological Behaviors in Data-to-Text Generation"
date:   2021-02-17 20:00:00 +0200
categories: nlp-summaries
permalink : "nlp-summaries/parenting-via-model-agnostic-rl"
tags: ["Depth: 1","data2text", "generation"]
mathjax : true
---

{% include scripts.html %}

[LINK TO THE PAPER](https://arxiv.org/pdf/2010.10866.pdf)

> Using Reinforcement Learning optimizing the PARENT metric ([summary](https://cfierro94.github.io/nlp-summaries/parent-metric)) leads to less omissions and hallucinations.

## Main idea
They state that the data contains these two problems (hallucinations and omissions): (1) the target text contains elements that are not grounded in the structured data (due to automatic creation of the datasets), and (2) some datasets expect the text to use all table fields and others to only contain some. So if we train the model by maximizing likelihood, we're forcing it to mimic the human behaviour and thus to mimic these dataset failures/inconsistencies.

They achieve SOTA in PARENT scores on WebNLG and WikiBio (with BLEU scores on par with previous SOTA).

## Related work
[Dusek et al. (2019)](https://www.aclweb.org/anthology/W19-8652.pdf) showed that cleaned data can significantly improve system ability to produce fact-accurate text.
