---
layout: post
title:  "Handling Divergent Reference Texts when Evaluating Table-to-Text Generation"
date:   2021-02-21 20:00:00 +0200
categories: nlp-summaries
permalink : "nlp-summaries/parent-metric"
tags: ["Depth: 4","data2text", "metrics"]
mathjax : true
---

{% include scripts.html %}

[LINK TO PAPER](https://arxiv.org/pdf/1906.01081.pdf)

> They proposed a new metric called PARENT that uses a union of the target text and the table, to reward correct information missing from the reference. They showed that this metric correlates better with human judgment compared to BLEU.

PARENT = **P**recision **A**nd **R**ecall of **E**ntailed **N**grams from the **T**able.

### D2T task definition
Given a table $$T = \{r_k\}_{k=1}^K$$ with records $$r=(\text{entity, attribute, value})$$ we want to generate a text $$G$$ that summarizes $$T$$. We will evaluate $$G$$ by comparing it to a reference text $$R$$. Thus, a set of tables to evaluate is the set $$D_M=\{T^i,G^i,R^i\}_{i=1}^N$$.

$$G^i_n$$ and $$R^i_n$$ represent the set of n-grams in $$G^i$$ and $$R^i$$ respectively, and so $$\#_{R^i_n}(g)$$ represents how many n_grams ''$$g$$'' there is in $$R^i_n$$.

### Metric definition
It computes the recall and precision of the generated text $$G$$ against both $$T$$ and $$R$$ (not only the text contrary to BLEU).

#### Entailment Probability $$w(g)$$
It's the probability than an n-gram in $$G$$ is correct given $$T$$. This $$P$$ is estimated by:
1. Word overlap: the sum of tokens ($$g_j$$) that are contained in $$T$$.
2. Co-ocurrence: the geometric average of the probability that a token was produced by one or more words in $$T$$, this probability is estimated using co-occurrence counts from a training set of table-reference pairs. In a formula that is:

$$
\begin{align}
&w(g) = \big(\prod _{j=1}^n P(g_j \Leftarrow T^i)\big)^{1/n} \\[5pt]
&\text{where} \quad P(g_j \Leftarrow T^i) = \max_{v\in \overline{T}^i} P(g_j \Leftarrow v)\\[5pt]
&\text{where} \quad P(g_j \Leftarrow v) = \frac{|\{(v \in T \wedge g_j \in R)\}|}{|\{T,R\}|}
\end{align}
$$

#### Entailment Precision.
It's the fraction of correct n-grams in $$G^i_n$$. An n-gram is correct if it occurs in $$R^i_n$$ or if it's highly probable to be entailed by $$T$$, so what we do to compute the entailment precision is to sum the probability of the n-gram being in the reference and the probability of it being entailed by the table (weighted by the probability  of the n-gram not being in $$R^i_n$$), and this sum is weighted by the times $$g$$ (this n-gram) appears in $$G^i_n$$ and normalized accordingly. In a formula:

$$
\begin{align}
&E_p^n = \frac{\sum_{g\in G_n^i} [P(g \in R^i_n) + P(g \notin R^i_n)w(g)]\#_{G^i_n(g)}}{\sum_{g\in G_i^n}\#_{G^i_n(g)}}\\[5pt]
&\text{where} \quad P(g \in R^i_n) = \frac{\#_{G^i_n(g),R^i_n(g)}}{\#_{G^i_n(g)}}
\end{align}
$$

Note that $$P(g \in R^i_n)$$ rewards the score for each time the n-gram appears in both the reference and the generation.

Then the final entailment precision is the combination of n-grams 1-4 with a geometric average:

$$
E_p = \Big(\prod_{n=1}^4 E_p^n\Big)^\frac{1}{4} \iff \exp\Big(\sum_{n=1}^4\log E_p^n\Big)
$$

#### Entailment Recall.
It's the fraction of n-grams in $$R^i_n$$ and in the table that are contained in $$G^i_n$$, where the *and* is trade-off by a $$\lambda$$ parameter. Furthermore, we only want to consider as "TP" the n-grams that can be recalled from the table, thus we weigh by $$w(g)$$ to force references not grounded in the table to be excluded.

$$
\begin{align}
&E_r = E_r(R^i)^{(\lambda-1)}E_r(T^i)^{(\lambda)}\\[5pt]
\text{where} \quad &E_r^n(R^i) = \frac{\sum_{g \in R^i_n}\#_{G^i_n(g),R^i_n(g)}w(g)}{\sum_{g \in R^i_n}\#_{R^i_n(g)}w(g)} \quad \text{,}\\[5pt]
&E_r(T^i) = \frac{1}{K}\sum_{k=1}^K\frac{1}{|\overline{r}_k|}LCS(\overline{r}_k, G^i)
\end{align}
$$

Where $$\overline{r}_k$$ is the string value of the records in $$T$$, and $$LCS$$ is the longest common substring which ensures that entity names appear in the same order in the text as in the table. Also, $$E_r^n(R^i)$$ is combined with a geometric average for n-grams with n=1-4.

Fianlly, precision and recall are combined in an F-score which is the PARENT metric.
