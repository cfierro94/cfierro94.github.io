---
layout: post
title:  "Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches"
date:   2021-02-07 20:00:00 +0200
categories: nlp-summaries
permalink : "nlp-summaries/can-small-and-synthetic-benchmarks-drive-modeling-innovation"
tags: ["Depth: 4","datasets"]
---
{% include scripts.html %}

[LINK TO THE PAPER](https://arxiv.org/abs/1810.04805)

> Small and carefully designed synthetic benchmarks (not created through crowdsourcing ðŸ’¸) may be useful as benchmarks for driving the development of new modeling approaches, instead of needing large and naturally created datasets.

## Main idea
We do need natural and big datasets for models that will be used in production, but for making modeling progress (i.e. to compare if an architecture is better than other) we could maybe use automatically generated datasets (through a database, or using some big text corpora).

They showed that many human-constructed benchmarks (NewsQA, NaturalQuestions, HotpotQA, ...) have high concurrence with SQuAD. They also constructed synthetic benchmarks that do not resemble natural language, yet have high concurrence with SQuAD, demonstrating that <u>naturalness</u> and <u>size</u> are not necessary for reflecting historical modeling improvements on SQuAD.

## Concurrence
Concurrence is used in the paper to compare datasets and to be able to say: if two datasets have high concurrence then we could compare modeling decision with either of them, e.g. SQuAD has high concurrence with $$X$$, and $$X$$ is way cheaper to produce so we might have achieved similar progress using dataset $$X$$ as benchmark.

#### Definition
Two datasets have high concurrence if they rank a set of models similarly, when train on the dataset train set and evaluated on the dataset test set.

Mathematically, concurrence is the correlation between the evaluations obtained in two different datasets for the same model.

They used 2 correlation functions:
1. Measures that both evaluations have a linear relation ($$\implies$$ ideally, if one model is $$k$$ times better than other in dataset 1 it should be $$m*k$$ better in dataset 2, with $$m$$ a constant for all the model evaluations).
1. Measures that both rankings (order) are similar.

## Synthetic datasets
### FuzzySyntheticQA
- Passage: random 150 tokens (from a token vocabulary).
- Answer: random token from the passage.
- Question: noisy cloze generation of the 10 token window of the answer token. Cloze means that the answer token is masked out, and it's noisy because they applied noisy "operations" to  the window: token replacement + token permutation + word dropout.

**Results.** It has high concurrence with SQuAD on non-pretrained modeling approaches. This is non trivial, since existing synthetic benchmarks ([bAbI task suite](https://arxiv.org/abs/1502.05698)) have low concurrence with SQuAD.

### WikidataSyntheticQA
Motivation: build synthetic cloze benchmarks that do not resemble natural language, yet require some of the reasoning capabilities necessary to handle complex natural language phenomena in SQuAD.

 Derived from Wikidata triples ([subject, predicate, object]). Wikidata contains entities (subject/object) that relate to each other (predicate). Each entity has a label (its main name) and aliases (alternative names). They picked one entity and all the entities related to that one, then they sampled 50 triples to use.
 - Passage: It's the concatenation of all 50 triples (separated by ".") replacing the entity with the label or an alias. So to have a "text" version of the triple.
 - Answer: Random element of a triple.
 - Question: the triple that contains the answer, with the answer element masked and randomly transforming it by:
    - Replacing the predicate with the inverse and then inverting the subject and the object, e.g.: `(Mae_C._Jemison, employer, [MASK])` $$\rightarrow$$ `([MASK], employee, Mae_C._Jemison)`.
    - Replace the unmasked entity with its hypernyms (e.g. NASA $$\rightarrow$$ space agency).

#### Results
- It has high concurrence with SQuAD.
-This result indicates that naturalness is not a necessary quality of benchmarks with high concurrence with SQuAD.
- They think that this high correlation may be caused by  the fact that a model needs to reason about.
- The Wikidata aliases provide great lexical complexity  to the benchmark, such that the benchmark is not trivially solvable through string pattern-matching (removing aliases from the generation procedure results in near-perfect performance from all modeling approaches).

<h1 class="post-thoughts"><span>Questions/Thoughts</span></h1>
- Did their synthetic benchmarks work well because they knew the SQuAD dataset and knew which capacities they needed to measure?
  - Do we generally know which capabilities we want to measure?
- I think this is an interesting idea to directly prove capabilities of a model. E.g. if I want to prove that my model understands antonyms I can build a synthetic dataset that contains that and test it. It's good because it'd be more specific and more controlled, I'm sure at what level the model is good. Naturally generated datasets are created by humans, from whatever they thought at the moment or  understood from the task that was asked from them.
- It's important (and maybe difficult to measure) to create a dataset that contains the lexical properties that one wants, while not being trivially solvable.
- On the negative  side, benchmarks inspire problems/issues of current approaches. Having a manually created dataset wouldn't create this inspiration.
